{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1288991,"sourceType":"datasetVersion","datasetId":744359}],"dockerImageVersionId":30302,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Denoising and Augmenting Medical Images for Better Machine Learning Models**\n\n## Minds behind this notebook\n* [Ikjot Singh](https://www.kaggle.com/ikjotsingh221) \n* [Prisha Sawhney](https://www.kaggle.com/prishasawhney)\r\n\r\n## **Overview**\r\nMedical imaging is a cornerstone of modern healthcare, aiding in accurate diagnoses and treatment planning. However, one of the biggest challenges in leveraging machine learning for medical applications is the **limited availability of high-quality labeled datasets**. The sensitive nature of patient data and the high cost of annotations make large-scale data collection a formidable task.  \r\n\r\nThis notebook aims to address this challenge by employing **advanced image processing techniques and Generative Adversarial Networks (GANs)** to:\r\n1. **Denoise medical images** — enhancing their quality for better interpretability and training outcomes.\r\n2. **Augment datasets** — generating synthetic but realistic medical images to overcome the issue of insufficient data.  \r\n\r\n## **Key Objectives**\r\n- **Denoising**: Medical images, especially those from modalities like X-rays and MRIs, are often subject to noise due to equipment limitations or environmental factors. This notebook demonstrates how to preprocess these images to improve their usability.  \r\n- **Augmentation**: Using GANs, we generate high-quality synthetic images that mimic the properties of real medical data, providing an effective solution for data scarcity.  \r\n\r\n## **Why This Matters**\r\n- **Improved Model Performance**: Denoised and augmented datasets improve the robustness and accuracy of machine learning models, especially in critical tasks like disease diagnosis and severity assessment.  \r\n- **Bridging the Data Gap**: By leveraging GANs for synthetic data generation, this notebook contributes to bridging the gap between the growing demand for AI in medicine and the lack of sufficient data to train these systems.  \r\n- **Scalable Solutions**: The techniques showcased here can be adapted to various types of medical imaging data, making it a versatile approach to data enhancement.  \r\n\r\n## **Highlights**\r\n- Use of **autoencoders** for denoising noisy medical images.\r\n- Implementation of **state-of-the-art GANs** to generate synthetic medical images.\r\n- Evaluation of generated data quality using metrics like **PSNR (Peak Signal-to-Noise Ratio)** and **SSIM (Structural Similarity Index)**.\r\n- Seamless integration of denoising and augmentation pipelines for medical datasets.\r\n\r\n## **Target Audience**\r\nThis notebook is designed for:\r\n- **AI researchers** working in the healthcare domain.\r\n- **Data scientists and ML practitioners** facing challenges with small medical datasets.\r\n- **Healthcare professionals and radiologists** interested in understanding how AI can enhance medical imaging analysis.\r\n\r\n---\r\n\r\nFollow along as we explore how cutting-edge techniques can transform noisy, limited medical datasets into robust resources for building impactful machine learning models.  \r\n","metadata":{}},{"cell_type":"markdown","source":"# Phase 1: Denoising the Images","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Import all the necessary libraries","metadata":{}},{"cell_type":"code","source":"# General-purpose libraries\nimport os\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom PIL import Image\n\n# Suppress TensorFlow warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nwarnings.filterwarnings('ignore')\n\n# Scikit-learn for data splitting and metrics\nfrom sklearn.model_selection import train_test_split\nfrom skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n\n# TensorFlow and Keras for deep learning\nimport tensorflow as tf\n\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, BatchNormalization, MaxPooling2D, UpSampling2D, Add, Dense, Flatten, \n    Reshape, Conv2DTranspose, Activation, LeakyReLU, Dropout, Resizing\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import VGG16\ntry:\n    from tensorflow.keras.optimizers import Adam\nexcept:\n    from keras.optimizers import Adam\n\n# Keras (standalone) for additional deep learning layers and models\nfrom keras.models import Sequential\nfrom keras.layers import concatenate\n\n# PyTorch for custom neural network models and training\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\n","metadata":{"execution":{"iopub.status.busy":"2024-11-28T23:14:06.757338Z","iopub.execute_input":"2024-11-28T23:14:06.757808Z","iopub.status.idle":"2024-11-28T23:14:14.925452Z","shell.execute_reply.started":"2024-11-28T23:14:06.757718Z","shell.execute_reply":"2024-11-28T23:14:14.924660Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Loading the dataset","metadata":{}},{"cell_type":"code","source":"image_paths = [os.path.join(r'/kaggle/input/medical-image-dataset/Dataset', fname) for fname in os.listdir(r'/kaggle/input/medical-image-dataset/Dataset')]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:14.926951Z","iopub.execute_input":"2024-11-28T23:14:14.927481Z","iopub.status.idle":"2024-11-28T23:14:14.942399Z","shell.execute_reply.started":"2024-11-28T23:14:14.927451Z","shell.execute_reply":"2024-11-28T23:14:14.941669Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Preparing the dataset\r\nThis section focuses on preparing a dataset of clean and noisy images for training and evaluation purposes. The prepare_dataset function processes a list of image paths, resizes the images to a specified dimension (default is 128x128), and normalizes their pixel values to the range [0, 1]. It also introduces Gaussian noise to the images using the add_gaussian_noise function.\r\n\r\nThe add_gaussian_noise function generates random noise following a Gaussian distribution, which is scaled by an alpha factor before being added to the original image. The noisy image values are clipped to ensure they remain within the valid pixel intensity range.\r\n\r\nThe result is two sets of images:\r\n\r\n- Clean images: Original images normalized to [0, 1].\r\n- Noisy images: Corresponding images with added Gaussian noise, also normalized to [0, 1].\r\n\r\nBoth datasets are returned with an added channel dimension to be compatible with deep learning models.","metadata":{}},{"cell_type":"code","source":"# Function to add Gaussian noise\ndef add_gaussian_noise(image, mean=0, stddev=1, alpha=1):\n    noise = np.random.normal(mean, stddev, image.shape).astype(np.float32)\n    noisy_image = image + noise*alpha\n    noisy_image = np.clip(noisy_image, 0, 255)  # Clip values to valid range\n    return noisy_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:14.943566Z","iopub.execute_input":"2024-11-28T23:14:14.943838Z","iopub.status.idle":"2024-11-28T23:14:14.950224Z","shell.execute_reply.started":"2024-11-28T23:14:14.943814Z","shell.execute_reply":"2024-11-28T23:14:14.949289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Trying out the noise function on the first image of the dataset\nimage = cv2.imread(r'/kaggle/input/medical-image-dataset/Dataset/1.jpg', cv2.IMREAD_GRAYSCALE)  # Replace with your image path\nimage = cv2.resize(image, (128, 128))  # Resize for model input compatibility\n\n# Generate noisy images\ngaussian_noisy_image = add_gaussian_noise(image,0,1,5)\n\n\n# Display original and noisy images\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 3, 1), plt.title(\"Original\"), plt.imshow(image, cmap='gray')\nplt.subplot(1, 3, 2), plt.title(\"Gaussian Noise\"), plt.imshow(gaussian_noisy_image, cmap='gray')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:14.952375Z","iopub.execute_input":"2024-11-28T23:14:14.952633Z","iopub.status.idle":"2024-11-28T23:14:15.305004Z","shell.execute_reply.started":"2024-11-28T23:14:14.952609Z","shell.execute_reply":"2024-11-28T23:14:15.303948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset and add noise\ndef prepare_dataset(image_paths, noise_function, resize_dim=(128,128)):\n    clean_images = []\n    noisy_images = []\n    for path in image_paths:\n        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n        image = cv2.resize(image, resize_dim)\n        clean_images.append(image)\n        noisy_images.append(noise_function(image, 0,1,5))\n    clean_images = np.array(clean_images).astype('float32') / 255.0  # Normalize to [0, 1]\n    noisy_images = np.array(noisy_images).astype('float32') / 255.0  # Normalize to [0, 1]\n    return clean_images[..., np.newaxis], noisy_images[..., np.newaxis]  # Add channel dimension","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:15.306096Z","iopub.execute_input":"2024-11-28T23:14:15.306401Z","iopub.status.idle":"2024-11-28T23:14:15.312420Z","shell.execute_reply.started":"2024-11-28T23:14:15.306354Z","shell.execute_reply":"2024-11-28T23:14:15.311492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_images, noisy_images = prepare_dataset(image_paths, add_gaussian_noise)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:15.313527Z","iopub.execute_input":"2024-11-28T23:14:15.313787Z","iopub.status.idle":"2024-11-28T23:14:16.856938Z","shell.execute_reply.started":"2024-11-28T23:14:15.313763Z","shell.execute_reply":"2024-11-28T23:14:16.856035Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(noisy_images, clean_images, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:16.857984Z","iopub.execute_input":"2024-11-28T23:14:16.858234Z","iopub.status.idle":"2024-11-28T23:14:16.868204Z","shell.execute_reply.started":"2024-11-28T23:14:16.858204Z","shell.execute_reply":"2024-11-28T23:14:16.867497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Shape of X_train: {X_train.shape}\")\nprint(f\"Shape of X_val: {X_val.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:16.869530Z","iopub.execute_input":"2024-11-28T23:14:16.869805Z","iopub.status.idle":"2024-11-28T23:14:16.877618Z","shell.execute_reply.started":"2024-11-28T23:14:16.869780Z","shell.execute_reply":"2024-11-28T23:14:16.876717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_list =[]\nrows = []\nModel_analysis = pd.DataFrame(columns=['Model Name', 'PSNR', 'SSIM'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:16.878584Z","iopub.execute_input":"2024-11-28T23:14:16.878908Z","iopub.status.idle":"2024-11-28T23:14:16.891512Z","shell.execute_reply.started":"2024-11-28T23:14:16.878883Z","shell.execute_reply":"2024-11-28T23:14:16.890689Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Creating a few helper functions\r\n\r\n\r\n### PSNR and SSIM: Evaluation Metrics for Image Quality\r\n\r\nWhen evaluating image quality, especially in tasks like image denoising or compression, **Peak Signal-to-Noise Ratio (PSNR)** and **Structural Similarity Index (SSIM)** are commonly used metrics.  \r\n\r\n\r\n#### **1. Peak Signal-to-Noise Ratio (PSNR)**\r\n\r\nPSNR measures the ratio between the maximum possible pixel value and the distortion introduced in the image. It quantifies image reconstruction quality, with higher values indicating better quality.\r\n\r\n\r\n#### **2. Structural Similarity Index (SSIM)**\r\nSSIM measures the perceptual similarity between two images based on luminance, contrast, and structural information. It accounts for human visual perception, making it more aligned with perceived image quality.\r\n\r\n#### **Key Differences**:\r\n- **PSNR**: Focuses purely on numerical differences between pixel intensities.\r\n- **SSIM**: Emphasizes structural and perceptual similarity, providing a better approximation of human visual judgment. \r\n\r\nHigher values of PSNR and SSIM indicate better image quality and closer similarity to the original image.","metadata":{}},{"cell_type":"code","source":"# Function for evaluating PSNR and SSIM\ndef evaluate_psnr_ssim(original_images, denoised_images):\n    psnr_values = []\n    ssim_values = []\n    \n    for original, denoised in zip(original_images, denoised_images):\n        # Compute PSNR\n        psnr_value = psnr(original, denoised, data_range=1.0)  # Normalized images, range [0, 1]\n        psnr_values.append(psnr_value)\n        \n        # Compute SSIM\n        ssim_value = ssim(original.squeeze(), denoised.squeeze(), data_range=1.0)  # Grayscale (squeeze for 2D)\n        ssim_values.append(ssim_value)\n    \n    # Print average metrics\n    print(f\"Average PSNR: {np.mean(psnr_values):.2f} dB\")\n    print(f\"Average SSIM: {np.mean(ssim_values):.4f}\")\n    return (np.mean(psnr_values), np.mean(ssim_values))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:16.894237Z","iopub.execute_input":"2024-11-28T23:14:16.894541Z","iopub.status.idle":"2024-11-28T23:14:16.902413Z","shell.execute_reply.started":"2024-11-28T23:14:16.894517Z","shell.execute_reply":"2024-11-28T23:14:16.901631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize predictions on the first 5 validation set images\ndef visualize_results(model, X_val, y_val, num_images=5):\n    fig, axes = plt.subplots(num_images, 4, figsize=(20, 6 * num_images))\n    fig.suptitle(\"Model Results: Original vs. Noisy vs. Denoised\", fontsize=16)\n\n    for i in range(num_images):\n        # Original, Noisy, and Denoised images\n        original = y_val[i]\n        noisy = X_val[i]\n        denoised = model.predict(noisy[np.newaxis, ...])[0]\n\n        # Metrics\n        psnr_score = psnr(original, denoised, data_range=1)\n        ssim_score = ssim(original, denoised, data_range=1, win_size=5, channel_axis=-1)  # Adjust win_size\n\n        # Plotting\n        axes[i, 0].imshow(original, cmap='gray')\n        axes[i, 0].set_title(f\"Original\")\n        axes[i, 0].axis('off')\n\n        axes[i, 1].imshow(noisy, cmap='gray')\n        axes[i, 1].set_title(f\"Noisy\")\n        axes[i, 1].axis('off')\n\n        axes[i, 2].imshow(denoised, cmap='gray')\n        axes[i, 2].set_title(f\"Denoised\\nPSNR: {psnr_score:.2f}, SSIM: {ssim_score:.3f}\")\n        axes[i, 2].axis('off')\n\n        # Difference (optional for error visualization)\n        diff = abs(original - denoised)\n        axes[i, 3].imshow(diff, cmap='hot')\n        axes[i, 3].set_title(f\"Difference\")\n        axes[i, 3].axis('off')\n\n    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit the title\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:16.903490Z","iopub.execute_input":"2024-11-28T23:14:16.903818Z","iopub.status.idle":"2024-11-28T23:14:16.916118Z","shell.execute_reply.started":"2024-11-28T23:14:16.903785Z","shell.execute_reply":"2024-11-28T23:14:16.915345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to evaluate and visualize results for vision transformers\ndef visualize_test_results(model, image_path, transform, mean, stddev, alpha, device):\n    # Load and preprocess the clean image\n    clean_img = Image.open(image_path).convert(\"L\")\n    clean_img = np.array(clean_img, dtype=np.float32)\n    noisy_img = add_gaussian_noise(clean_img, mean=mean, stddev=stddev, alpha=alpha)\n\n    # Convert images to PyTorch tensors\n    clean_img_tensor = transform(Image.fromarray(clean_img.astype(np.uint8))).unsqueeze(0).to(device)\n    noisy_img_tensor = transform(Image.fromarray(noisy_img.astype(np.uint8))).unsqueeze(0).to(device)\n\n    # Denoise with the model\n    model.eval()\n    with torch.no_grad():\n        denoised_img_tensor = model(noisy_img_tensor)\n\n    # Convert tensors to numpy arrays\n    denoised_img = denoised_img_tensor.squeeze(0).squeeze(0).cpu().numpy()\n    clean_img = clean_img_tensor.squeeze(0).squeeze(0).cpu().numpy()\n\n    # Compute metrics\n    psnr_score = psnr(clean_img, denoised_img, data_range=1.0)\n    ssim_score = ssim(clean_img, denoised_img, data_range=1.0)\n\n    # Plot results\n    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n    axes[0].imshow(clean_img, cmap=\"gray\")\n    axes[0].set_title(\"Original\")\n    axes[0].axis(\"off\")\n\n    axes[1].imshow(noisy_img, cmap=\"gray\")\n    axes[1].set_title(\"Noisy\")\n    axes[1].axis(\"off\")\n\n    axes[2].imshow(denoised_img, cmap=\"gray\")\n    axes[2].set_title(f\"Denoised\\nPSNR: {psnr_score:.2f} dB, SSIM: {ssim_score:.4f}\")\n    axes[2].axis(\"off\")\n\n    axes[3].imshow(np.abs(clean_img - denoised_img), cmap=\"hot\")\n    axes[3].set_title(\"Difference\")\n    axes[3].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:16.917089Z","iopub.execute_input":"2024-11-28T23:14:16.917348Z","iopub.status.idle":"2024-11-28T23:14:16.931107Z","shell.execute_reply.started":"2024-11-28T23:14:16.917294Z","shell.execute_reply":"2024-11-28T23:14:16.930362Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Let's dive into analyzing a few architectures\n\n\r\n\r\n### 1. Deep Autoencoder Architecture for 128x128 Grayscale Images\r\n\r\nThis code defines a **convolutional autoencoder** designed for denoising grayscale images of size 128x128. The architecture consists of two main components:\r\n\r\n1. **Encoder**:\r\n   - Extracts features while progressively reducing the spatial dimensions using **Conv2D**, **BatchNormalization**, and **MaxPooling2D** layers.\r\n\r\n2. **Decoder**:\r\n   - Reconstructs the image from the encoded representation, upsampling the spatial dimensions using **Conv2D**, **BatchNormalization**, and **UpSampling2D** layers.\r\n\r\n---\r\n\r\n#### **Architecture Details**\r\n- **Total Layers**: 14 layers (7 Conv2D layers, 4 BatchNormalization layers, 3 Pooling/Upsampross all layers.\r\n\r\nThe model is compiled with the **Adam optimizer** and uses **Mean Squared Error (MSE)** as the loss function for training.","metadata":{}},{"cell_type":"code","source":"model_name = 'Deep Autoencoder'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:16.932054Z","iopub.execute_input":"2024-11-28T23:14:16.932416Z","iopub.status.idle":"2024-11-28T23:14:16.944205Z","shell.execute_reply.started":"2024-11-28T23:14:16.932391Z","shell.execute_reply":"2024-11-28T23:14:16.943523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def autoencoder_deep_128():\n    input_img = Input(shape=(128, 128, 1), name='image_input')\n\n    # Encoder\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='Conv1')(input_img)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2), padding='same', name='pool1')(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='Conv2')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2), padding='same', name='pool2')(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='Conv3')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2), padding='same', name='pool3')(x)\n\n    # Decoder\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='Conv4')(x)\n    x = BatchNormalization()(x)\n    x = UpSampling2D((2, 2), name='upsample1')(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='Conv5')(x)\n    x = BatchNormalization()(x)\n    x = UpSampling2D((2, 2), name='upsample2')(x)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='Conv6')(x)\n    x = BatchNormalization()(x)\n    x = UpSampling2D((2, 2), name='upsample3')(x)\n    output_img = Conv2D(1, (3, 3), activation='sigmoid', padding='same', name='Conv7')(x)\n\n    # Model\n    autoencoder = Model(inputs=input_img, outputs=output_img)\n    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n    \n    return autoencoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:16.945157Z","iopub.execute_input":"2024-11-28T23:14:16.945507Z","iopub.status.idle":"2024-11-28T23:14:16.957291Z","shell.execute_reply.started":"2024-11-28T23:14:16.945481Z","shell.execute_reply":"2024-11-28T23:14:16.956587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"autoencoder_deep = autoencoder_deep_128()\nautoencoder_deep.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:16.958191Z","iopub.execute_input":"2024-11-28T23:14:16.958485Z","iopub.status.idle":"2024-11-28T23:14:21.919185Z","shell.execute_reply.started":"2024-11-28T23:14:16.958461Z","shell.execute_reply":"2024-11-28T23:14:21.918244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = autoencoder_deep.fit(\n    X_train, y_train, batch_size=1,\n    epochs=100,\n    validation_data=(X_val, y_val),\n    verbose=1\n)\nmodel_list.append(autoencoder_deep)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:14:21.920627Z","iopub.execute_input":"2024-11-28T23:14:21.920891Z","iopub.status.idle":"2024-11-28T23:15:17.802135Z","shell.execute_reply.started":"2024-11-28T23:14:21.920867Z","shell.execute_reply":"2024-11-28T23:15:17.801339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate denoised images\ndenoised_images = autoencoder_deep.predict(X_val)\n\n# Evaluate PSNR and SSIM\ns1, s2 = evaluate_psnr_ssim(y_val, denoised_images)\nrows.append({'Model Name': model_name, 'PSNR': s1, 'SSIM': s2})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:17.803445Z","iopub.execute_input":"2024-11-28T23:15:17.804253Z","iopub.status.idle":"2024-11-28T23:15:18.402617Z","shell.execute_reply.started":"2024-11-28T23:15:17.804206Z","shell.execute_reply":"2024-11-28T23:15:18.401587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_results(autoencoder_deep, X_val, y_val, num_images=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:18.404142Z","iopub.execute_input":"2024-11-28T23:15:18.404825Z","iopub.status.idle":"2024-11-28T23:15:20.587480Z","shell.execute_reply.started":"2024-11-28T23:15:18.404783Z","shell.execute_reply":"2024-11-28T23:15:20.586062Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. CNN-Based Denoising Model with PyTorch\r\n\r\nThis code implements a convolutional neural network (CNN) for denoising grayscale medical images. The pipeline includes the following components:\r\n\r\n1. **Model Architecture**: \r\n   - The CNN is divided into:\r\n     - **Encoder**: Two convolutional layers (64 filters, 3x3 kernel) with ReLU activation.\r\n     - **Decoder**: Two convolutional layers (64 filters for intermediate, 1 filter for output) with ReLU activation.\r\n\r\n2. **Dataset Preparation**:\r\n   - Images are loaded from a specified folder and split into training and test datasets.\r\n   - Gaussian noise is added to create noisy images for training.\r\n\r\n3. **Training**:\r\n   - The model is 2rained for **40 epochs** using the **Adam optimizer** and **Mean Squared Error (MSE)** loss function.\r\n   - Training is performed on batches of size 16.\r\n\r\n4. **Visualization**:\r\n   - The results of denoising are visualized on a sampl have shown promising results.","metadata":{}},{"cell_type":"code","source":"model_name = \"CNN-Denoising\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:20.588887Z","iopub.execute_input":"2024-11-28T23:15:20.589166Z","iopub.status.idle":"2024-11-28T23:15:20.593209Z","shell.execute_reply.started":"2024-11-28T23:15:20.589140Z","shell.execute_reply":"2024-11-28T23:15:20.592365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_denoising_cnn():\n    encoder = nn.Sequential(\n        nn.Conv2d(1, 64, kernel_size=3, padding=1),\n        nn.ReLU(),\n        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n        nn.ReLU(),\n    )\n    decoder = nn.Sequential(\n        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n        nn.ReLU(),\n        nn.Conv2d(64, 1, kernel_size=3, padding=1),\n    )\n    return nn.Sequential(encoder, decoder)\n\n# Training function\ndef train_model(model, dataloader, optimizer, criterion, device, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for noisy_imgs, clean_imgs in dataloader:\n            noisy_imgs, clean_imgs = noisy_imgs.to(device), clean_imgs.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(noisy_imgs)\n            loss = criterion(outputs, clean_imgs)\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader):.4f}\")\n\n\n# Paths and configuration\nimage_folder = \"/kaggle/input/medical-image-dataset/Dataset\"  # Replace with the folder path\nall_image_paths = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder) if fname.endswith(\".jpg\")]\n\n# Split dataset\ntrain_image_paths, test_image_paths = train_test_split(all_image_paths, test_size=0.2, random_state=42)\n\n# Device and transformation setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntransform = transforms.Compose([transforms.ToTensor()])\n\n# Dataset preparation\ntrain_clean, train_noisy = prepare_dataset(train_image_paths, add_gaussian_noise)\ntrain_dataset = torch.utils.data.TensorDataset(torch.tensor(train_noisy).permute(0, 3, 1, 2), torch.tensor(train_clean).permute(0, 3, 1, 2))\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n# Initialize model, optimizer, and loss function\nmodel = create_denoising_cnn().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\n# Train the model\ntrain_model(model, train_loader, optimizer, criterion, device, epochs=20)\n\n# Test and visualize results\nfor i, img_path in enumerate(test_image_paths[:5]):  # Visualize first 5 test results\n    visualize_test_results(model, img_path, transform, mean=0, stddev=1, alpha=5, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:20.594536Z","iopub.execute_input":"2024-11-28T23:15:20.594803Z","iopub.status.idle":"2024-11-28T23:15:26.256805Z","shell.execute_reply.started":"2024-11-28T23:15:20.594778Z","shell.execute_reply":"2024-11-28T23:15:26.255921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_list.append(model)\nmodel.eval()\n\n# Ensure input data is in the correct format\nX_val_tensor = torch.tensor(X_val).permute(0, 3, 1, 2).to(device)\n\n# Disable gradient computation for inference\nwith torch.no_grad():\n    denoised_images = model(X_val_tensor)\n\n# Move denoised images back to CPU and convert to NumPy for evaluation\ndenoised_images = denoised_images.permute(0, 2, 3, 1).cpu().numpy()\ns1, s2 = evaluate_psnr_ssim(y_val, denoised_images)\nrows.append({'Model Name': model_name, 'PSNR': s1, 'SSIM': s2})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:26.258149Z","iopub.execute_input":"2024-11-28T23:15:26.258946Z","iopub.status.idle":"2024-11-28T23:15:26.319015Z","shell.execute_reply.started":"2024-11-28T23:15:26.258905Z","shell.execute_reply":"2024-11-28T23:15:26.318223Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Multiscale Autoencoder Architecture Overview\r\n\r\nThis code defines a **multiscale autoencoder** for image processing, designed to reconstruct input images with improved feature extraction through the use of skip connections. Here's a breakdown of the architecture:\r\n\r\n- **Input Layer**: Takes grayscale images of size 128x128 pixels with a single channel.\r\n- **Encoder**:\r\n  - **First block**: Convolution with 64 filters and 3x3 kernel size, followed by a max-pooling layer, reducing the spatial dimensions to 64x64.\r\n  - **Second block**: Convolution with 128 filters, followed by another max-pooling, further reducing dimensions to 32x32.\r\n  - **Third block**: Convolution with 256 filters and max-pooling, reducing dimensions to 16x16.\r\n- **Decoder**:\r\n  - **First block**: Up-sampling followed by convolution with 128 filters, reconstructing the feature map to 32x32.\r\n  - **Second block**: A skip connection concatenates the output with the corresponding encoder layer (32x32), followed by up-sampling and a convolution with 64 filters to reach 64x64.\r\n  - **Third block**: Another skip connection connects the output with the first encoder block (64x64), followed by up-sampling and convolution to reconstruct the final output size of 128x128.\r\n- **Output Layer**: A convolutional layer with 1 filter and a sigmoid activation function, producing the final denoised image with the shape (128x128, 1).\r\n\r\n### Total Number of Layers:\r\n- **Convolutional layers**: 8 layers (including encoder and decoder blocks)\r\n- **Pooling layers**: 3 max-pooling layers\r\n- **Up-sampling layers**: 3 layers\r\n- **Skip connections**: 2 concatenations\r\n\r\nThis multiscale architecture effectively captures both local and global features from the input image, allowing for enhanced reconstruction quality and detail retention.","metadata":{}},{"cell_type":"code","source":"model_name = \"Multiscale Autoencoder\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:26.320012Z","iopub.execute_input":"2024-11-28T23:15:26.320255Z","iopub.status.idle":"2024-11-28T23:15:26.324544Z","shell.execute_reply.started":"2024-11-28T23:15:26.320225Z","shell.execute_reply":"2024-11-28T23:15:26.323523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def autoencoder_multiscale_128():\n    input_img = Input(shape=(128, 128, 1))\n    \n    # Encoder\n    x1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n    x1 = MaxPooling2D((2, 2), padding='same')(x1)  # (64x64)\n    \n    x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n    x2 = MaxPooling2D((2, 2), padding='same')(x2)  # (32x32)\n    \n    x3 = Conv2D(256, (3, 3), activation='relu', padding='same')(x2)\n    x3 = MaxPooling2D((2, 2), padding='same')(x3)  # (16x16)\n    \n    # Decoder\n    y3 = UpSampling2D((2, 2))(x3)  # (32x32)\n    y3 = Conv2D(128, (3, 3), activation='relu', padding='same')(y3)\n    \n    y2 = concatenate([y3, x2])  # Skip connection\n    y2 = UpSampling2D((2, 2))(y2)  # (64x64)\n    y2 = Conv2D(64, (3, 3), activation='relu', padding='same')(y2)\n    \n    y1 = concatenate([y2, x1])  # Skip connection\n    y1 = UpSampling2D((2, 2))(y1)  # (128x128)\n    y1 = Conv2D(64, (3, 3), activation='relu', padding='same')(y1)\n    \n    # Output Layer\n    output_img = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(y1)  # Match (128x128, 1)\n    \n    # Model\n    autoencoder = Model(inputs=input_img, outputs=output_img)\n    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n    return autoencoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:26.325673Z","iopub.execute_input":"2024-11-28T23:15:26.325949Z","iopub.status.idle":"2024-11-28T23:15:26.579222Z","shell.execute_reply.started":"2024-11-28T23:15:26.325914Z","shell.execute_reply":"2024-11-28T23:15:26.578260Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"autoencoder_multiscale = autoencoder_multiscale_128()\nautoencoder_multiscale.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:26.580540Z","iopub.execute_input":"2024-11-28T23:15:26.580829Z","iopub.status.idle":"2024-11-28T23:15:26.667537Z","shell.execute_reply.started":"2024-11-28T23:15:26.580789Z","shell.execute_reply":"2024-11-28T23:15:26.666615Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = autoencoder_multiscale.fit(\n    X_train, y_train, batch_size=1,\n    epochs=50,\n    validation_data=(X_val, y_val),\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:26.668675Z","iopub.execute_input":"2024-11-28T23:15:26.668937Z","iopub.status.idle":"2024-11-28T23:15:51.192885Z","shell.execute_reply.started":"2024-11-28T23:15:26.668913Z","shell.execute_reply":"2024-11-28T23:15:51.191968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_list.append(autoencoder_multiscale)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:51.194476Z","iopub.execute_input":"2024-11-28T23:15:51.194870Z","iopub.status.idle":"2024-11-28T23:15:51.199714Z","shell.execute_reply.started":"2024-11-28T23:15:51.194832Z","shell.execute_reply":"2024-11-28T23:15:51.198682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate denoised images\ndenoised_images = autoencoder_multiscale.predict(X_val)\n\n# Evaluate PSNR and SSIM\ns1, s2 = evaluate_psnr_ssim(y_val, denoised_images)\nrows.append({'Model Name': model_name, 'PSNR': s1, 'SSIM': s2})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:51.200788Z","iopub.execute_input":"2024-11-28T23:15:51.201088Z","iopub.status.idle":"2024-11-28T23:15:51.758990Z","shell.execute_reply.started":"2024-11-28T23:15:51.201063Z","shell.execute_reply":"2024-11-28T23:15:51.758119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"visualize_results(autoencoder_multiscale, X_val, y_val, num_images=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:51.762641Z","iopub.execute_input":"2024-11-28T23:15:51.762909Z","iopub.status.idle":"2024-11-28T23:15:54.057889Z","shell.execute_reply.started":"2024-11-28T23:15:51.762874Z","shell.execute_reply":"2024-11-28T23:15:54.056835Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Residual Autoencoder Architecture Overview\r\n\r\nThis code defines a **residual autoencoder** for image processing, which incorporates skip (residual) connections within the decoder to enhance feature propagation and gradient flow. Here's a breakdown of the architecture:\r\n\r\n- **Input Layer**: Accepts grayscale images of size 128x128 pixels with a single channel.\r\n- **Encoder**:\r\n  - **First block**: A convolutional layer with 64 filters, a 3x3 kernel, and ReLU activation, followed by a max-pooling layer that reduces the spatial dimensions to 64x64.\r\n  - **Second block**: Another convolutional layer with 64 filters, a 3x3 kernel, and ReLU activation, followed by another max-pooling layer, further reducing the dimensions to 32x32.\r\n- **Decoder with Residual Connections**:\r\n  - **First block**: A convolutional layer with 64 filters, a 3x3 kernel, and ReLU activation, followed by up-sampling to 64x64.\r\n  - **Skip connection**: Adds the up-sampled feature map to the feature map from the encoder at the corresponding level (32x32).\r\n  - **Second block**: A convolutional layer with 64 filters and a 3x3 kernel, followed by up-sampling to 128x128.\r\n  - **Skip connection**: Adds the up-sampled feature map to the feature map from the first encoder block (64x64).\r\n- **Output Layer**: A final convolutional layer with 1 filter and a sigmoid activation function, generating the reconstructed image (128x128, 1).\r\n\r\n### Residual Connections:\r\n- **Purpose**: These connections allow the model to learn the residual mapping, improving the model's ability to reconstruct complex features and alleviating the vanishing gradient problem.\r\n- **Structure**: Two main skip connections are added in the decoder section, linking the up-sampled outputs with their corresponding encoder outputs.\r\n\r\n### Total Number of Layers:\r\n- **Convolutional layers**: 5 layers (including both encoder and decoder)\r\n- **Pooling layers**: 2 max-pooling layers\r\n- **Up-sampling layers**: 2 layers\r\n- **Skip connections**: 2 residual connections\r\n\r\nThis **residual autoencoder** architecture helps achieve better reconstruction performance by leveraging deep learning principles that improve information flow through the network, making it more effective for tasks like image denoising and enhancement.","metadata":{}},{"cell_type":"code","source":"model_name = \"Residual Autoencoder\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:54.059139Z","iopub.execute_input":"2024-11-28T23:15:54.059464Z","iopub.status.idle":"2024-11-28T23:15:54.063711Z","shell.execute_reply.started":"2024-11-28T23:15:54.059434Z","shell.execute_reply":"2024-11-28T23:15:54.062821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def autoencoder_residual():\n    input_img = Input(shape=(128, 128, 1), name='image_input')\n\n    # Encoder\n    x1 = Conv2D(64, (3, 3), activation='relu', padding='same', name='Conv1')(input_img)\n    x1_pool = MaxPooling2D((2, 2), padding='same', name='pool1')(x1)\n    x2 = Conv2D(64, (3, 3), activation='relu', padding='same', name='Conv2')(x1_pool)\n    x2_pool = MaxPooling2D((2, 2), padding='same', name='pool2')(x2)\n\n    # Decoder with Residual Connections\n    x3 = Conv2D(64, (3, 3), activation='relu', padding='same', name='Conv3')(x2_pool)\n    x3_upsample = UpSampling2D((2, 2), name='upsample1')(x3)\n    x3_add = Add()([x3_upsample, x2])  # Skip connection\n    x4 = Conv2D(64, (3, 3), activation='relu', padding='same', name='Conv4')(x3_add)\n    x4_upsample = UpSampling2D((2, 2), name='upsample2')(x4)\n    x4_add = Add()([x4_upsample, x1])  # Skip connection\n    output_img = Conv2D(1, (3, 3), activation='sigmoid', padding='same', name='Conv5')(x4_add)\n\n    # Model\n    autoencoder = Model(inputs=input_img, outputs=output_img)\n    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n    \n    return autoencoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:54.064986Z","iopub.execute_input":"2024-11-28T23:15:54.065252Z","iopub.status.idle":"2024-11-28T23:15:54.081107Z","shell.execute_reply.started":"2024-11-28T23:15:54.065219Z","shell.execute_reply":"2024-11-28T23:15:54.080150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build the model\nresdae_model = autoencoder_residual()\n\n# Train the model\nhistory = resdae_model.fit(\n    X_train, y_train, batch_size=1,\n    epochs=50,\n    validation_data=(X_val, y_val),\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:15:54.082296Z","iopub.execute_input":"2024-11-28T23:15:54.082579Z","iopub.status.idle":"2024-11-28T23:16:11.683374Z","shell.execute_reply.started":"2024-11-28T23:15:54.082555Z","shell.execute_reply":"2024-11-28T23:16:11.682609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate denoised images\n\nmodel_list.append(resdae_model)\ndenoised_images = resdae_model.predict(X_val)\n\n# Evaluate PSNR and SSIM\ns1, s2 = evaluate_psnr_ssim(y_val, denoised_images)\nrows.append({'Model Name': model_name, 'PSNR': s1, 'SSIM': s2})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:16:11.684546Z","iopub.execute_input":"2024-11-28T23:16:11.684793Z","iopub.status.idle":"2024-11-28T23:16:11.869432Z","shell.execute_reply.started":"2024-11-28T23:16:11.684769Z","shell.execute_reply":"2024-11-28T23:16:11.868155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the visualization function\nvisualize_results(resdae_model, X_val, y_val, num_images=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:16:11.870769Z","iopub.execute_input":"2024-11-28T23:16:11.871776Z","iopub.status.idle":"2024-11-28T23:16:14.214225Z","shell.execute_reply.started":"2024-11-28T23:16:11.871731Z","shell.execute_reply":"2024-11-28T23:16:14.212678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Model_analysis = pd.concat([Model_analysis, pd.DataFrame(rows)], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:16:14.215822Z","iopub.execute_input":"2024-11-28T23:16:14.216134Z","iopub.status.idle":"2024-11-28T23:16:14.222420Z","shell.execute_reply.started":"2024-11-28T23:16:14.216104Z","shell.execute_reply":"2024-11-28T23:16:14.221440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Model_analysis.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:16:14.223621Z","iopub.execute_input":"2024-11-28T23:16:14.223910Z","iopub.status.idle":"2024-11-28T23:16:14.242347Z","shell.execute_reply.started":"2024-11-28T23:16:14.223883Z","shell.execute_reply":"2024-11-28T23:16:14.241581Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Model Selection for Denoising\r\n\r\nAfter evaluating multiple autoencoder architectures, wwill be using the model withed the best performance, as measured by PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). The higher scores indicate superior image quality and better reconstruction capabilities compared to other models testease.","metadata":{}},{"cell_type":"code","source":"# Best results are obtained from residual autoencoder\n# Denoising the entire dataset using resdae model\n\n# Define output directories\noutput_dir = \"/kaggle/working/denoised_dataset\"\n\n# Create directories if they do not exist\nos.makedirs(output_dir, exist_ok=True)\n\nmax_psnr_index = Model_analysis['PSNR'].idxmax()\n\n# Retrieve the corresponding model\nbest_model = model_list[max_psnr_index]\n\n# Output\nprint(\"The model with the highest PSNR is:\", Model_analysis.loc[max_psnr_index, 'Model Name'])\n\n# Function to denoise and save images one by one\ndef denoise_and_save_one_by_one(model, dataset, output_dir, prefix=\"img\"):\n    total_samples = dataset.shape[0]\n    for idx in range(total_samples):\n        # Get the current image and expand dimensions for prediction\n        image = np.expand_dims(dataset[idx], axis=0)\n        denoised_image = model.predict(image)  # Denoise the image\n        \n        # Normalize the denoised image to [0, 255]\n        denoised_image = (denoised_image.squeeze() * 255).astype(np.uint8)\n        \n        # Save the image\n        img_path = os.path.join(output_dir, f\"{prefix}_{idx + 1}.png\")\n        cv2.imwrite(img_path, denoised_image)\n\n# Denoise and save training images\ndenoise_and_save_one_by_one(best_model, X_train, output_dir, prefix = \"set1 \")\n\n# Denoise and save validation images\ndenoise_and_save_one_by_one(best_model, X_val, output_dir, prefix = \"set2 \")\n\nprint(f\"Denoised dataset saved in {output_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T23:16:14.243365Z","iopub.execute_input":"2024-11-28T23:16:14.243621Z","iopub.status.idle":"2024-11-28T23:16:18.525379Z","shell.execute_reply.started":"2024-11-28T23:16:14.243597Z","shell.execute_reply":"2024-11-28T23:16:18.524350Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Freeing up space (Optional)","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ndel image_paths, add_gaussian_noise, image, gaussian_noisy_image\ndel prepare_dataset, clean_images, noisy_images, X_train, X_val, y_train, y_val\ndel evaluate_psnr_ssim, visualize_results, visualize_test_results\ndel autoencoder_deep_128, autoencoder_deep, history, denoised_images\ndel create_denoising_cnn, train_model, image_folder, all_image_paths\ndel train_image_paths, test_image_paths, device, transform\ndel train_clean, train_noisy, train_dataset, train_loader\ndel model, optimizer, criterion, X_val_tensor\ndel autoencoder_multiscale_128, autoencoder_multiscale, autoencoder_residual, best_model, max_psnr_index\ndel resdae_model, denoise_and_save_one_by_one, model_list, Model_analysis, s1, s2, model_name, rows\nimport gc\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T21:53:02.489408Z","iopub.execute_input":"2024-11-28T21:53:02.489798Z","iopub.status.idle":"2024-11-28T21:53:02.823719Z","shell.execute_reply.started":"2024-11-28T21:53:02.489761Z","shell.execute_reply":"2024-11-28T21:53:02.822771Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Phase 2: Synthetically generating Dental X-Ray images for Augmenting the dataset using DCGANs","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Initializing necessary constants","metadata":{}},{"cell_type":"code","source":"NOISE_DIM = 100  \nBATCH_SIZE = 4 \nSTEPS_PER_EPOCH = 3750\nEPOCHS = 10\nSEED = 40\nWIDTH, HEIGHT, CHANNELS = 128, 128, 1\n\nOPTIMIZER = Adam(0.0002, 0.5)","metadata":{"id":"k-YEhz2NvqGm","execution":{"iopub.status.busy":"2024-11-28T21:53:02.825035Z","iopub.execute_input":"2024-11-28T21:53:02.825768Z","iopub.status.idle":"2024-11-28T21:53:02.830627Z","shell.execute_reply.started":"2024-11-28T21:53:02.825729Z","shell.execute_reply":"2024-11-28T21:53:02.829775Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAIN_DIR = \"/kaggle/working/denoised_dataset\"","metadata":{"execution":{"iopub.status.busy":"2024-11-28T21:53:02.832326Z","iopub.execute_input":"2024-11-28T21:53:02.832622Z","iopub.status.idle":"2024-11-28T21:53:02.845504Z","shell.execute_reply.started":"2024-11-28T21:53:02.832586Z","shell.execute_reply":"2024-11-28T21:53:02.844825Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Loading the denoised Dataset","metadata":{}},{"cell_type":"code","source":"def load_images(folder):\n    \n    imgs = []\n    target = 1\n    labels = []\n    for i in os.listdir(folder):\n        img_dir = os.path.join(folder,i)\n        try:\n            img = cv2.imread(img_dir)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            img = cv2.resize(img, (128,128))\n            imgs.append(img)\n            labels.append(target)\n        except:\n            continue\n        \n    imgs = np.array(imgs)\n    labels = np.array(labels)\n    \n    return imgs, labels","metadata":{"execution":{"iopub.status.busy":"2024-11-28T21:53:03.578541Z","iopub.execute_input":"2024-11-28T21:53:03.579499Z","iopub.status.idle":"2024-11-28T21:53:03.585042Z","shell.execute_reply.started":"2024-11-28T21:53:03.579463Z","shell.execute_reply":"2024-11-28T21:53:03.584185Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data, labels = load_images(MAIN_DIR)\ndata.shape, labels.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-28T21:53:04.017417Z","iopub.execute_input":"2024-11-28T21:53:04.017789Z","iopub.status.idle":"2024-11-28T21:53:04.061849Z","shell.execute_reply.started":"2024-11-28T21:53:04.017760Z","shell.execute_reply":"2024-11-28T21:53:04.060999Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.random.seed(SEED)\nidxs = np.random.randint(0, 120, 20)","metadata":{"execution":{"iopub.status.busy":"2024-11-28T21:53:04.409978Z","iopub.execute_input":"2024-11-28T21:53:04.410335Z","iopub.status.idle":"2024-11-28T21:53:04.415264Z","shell.execute_reply.started":"2024-11-28T21:53:04.410307Z","shell.execute_reply":"2024-11-28T21:53:04.414342Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = data[idxs]\nX_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-28T21:53:04.816833Z","iopub.execute_input":"2024-11-28T21:53:04.817736Z","iopub.status.idle":"2024-11-28T21:53:04.823550Z","shell.execute_reply.started":"2024-11-28T21:53:04.817671Z","shell.execute_reply":"2024-11-28T21:53:04.822575Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Preparing the dataset for further use in GANs","metadata":{}},{"cell_type":"code","source":"# Normalize the Images\nX_train = (X_train.astype(np.float32) - 127.5) / 127.5\n\n# Reshape images \nX_train = X_train.reshape(-1, WIDTH,HEIGHT,CHANNELS)\n\n# Check shape\nX_train.shape","metadata":{"id":"mE3l1VfUb_jg","execution":{"iopub.status.busy":"2024-11-28T21:53:06.085905Z","iopub.execute_input":"2024-11-28T21:53:06.086627Z","iopub.status.idle":"2024-11-28T21:53:06.094266Z","shell.execute_reply.started":"2024-11-28T21:53:06.086598Z","shell.execute_reply":"2024-11-28T21:53:06.093381Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nfor i in range(10):\n    axs = plt.subplot(2,5,i+1)\n    plt.imshow(X_train[i], cmap=\"gray\")\n    plt.axis('off')\n    axs.set_xticklabels([])\n    axs.set_yticklabels([])\n    plt.subplots_adjust(wspace=None, hspace=None)\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-11-28T21:53:06.675842Z","iopub.execute_input":"2024-11-28T21:53:06.676616Z","iopub.status.idle":"2024-11-28T21:53:07.495036Z","shell.execute_reply.started":"2024-11-28T21:53:06.676584Z","shell.execute_reply":"2024-11-28T21:53:07.494108Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Building the DCGANs Architecture\r\n\r\n### DCGAN Architecture Overview\r\n\r\nThe **DCGAN (Deep Convolutional Generative Adversarial Network)** consists of two main components: the **Generator** and the **Discriminator**, each playing a critical role in the training process to generate realistic images from random noise.\r\n\r\n#### Generator\r\nThe **Generator** takes a random latent vector (sampled from a normal distribution) as input and transforms it into an image through a series of transposed convolutional layers. This architecture learns to create increasingly detailed images as the model trains.\r\n\r\n- **Input**: Random noise (latent vector)\r\n- **Output**: Generated image\r\n- **Architecture**:\r\n  - Dense layer to reshape the noise into a (32, 32, 256) tensor.\r\n  - Three `Conv2DTranspose` layers, which upsample the input to generate an image, with LeakyReLU activations for non-linearity.\r\n  - Final `Conv2D` layer with `tanh` activation to produce the final output image.\r\n\r\n#### Discriminator\r\nThe **Discriminator** is a binary classifier that distinguishes between real images and those generated by the Generator. Its aim is to learn how to identify authentic images effectively.\r\n\r\n- **Input**: Real or generated image\r\n- **Output**: Probability indicating whether the image is real or fake\r\n- **Architecture**:\r\n  - Series of `Conv2D` layers with LeakyReLU activations for feature extraction.\r\n  - Dropout layer for regularization.\r\n  - Final `Dense` layer with a sigmoid activation function for binary classification.\r\n\r\n#### Training Process\r\nThe Generator and Discriminator are trained together in a competitive process, where the Generator tries to produce realistic images to fool the Discriminator, while the Discriminator aims to become better at identifying real images from fakes. This adversarial process improves both models until the Generator creates high-quality images indistinifferentiate from real images.","metadata":{}},{"cell_type":"code","source":"def build_generator():\n\n    \"\"\"\n        Generator model \"generates\" images using random noise. The random noise AKA Latent Vector\n        is sampled from a Normal Distribution which is given as the input to the Generator. Using\n        Transposed Convolution, the latent vector is transformed to produce an image\n        We use 3 Conv2DTranspose layers (which help in producing an image using features; opposite\n        of Convolutional Learning)\n\n        Input: Random Noise / Latent Vector\n        Output: Image\n    \"\"\"\n\n    model = Sequential([\n\n        Dense(32*32*256, input_dim=NOISE_DIM),\n        LeakyReLU(alpha=0.2),\n        Reshape((32,32,256)),\n        \n        Conv2DTranspose(128, (4, 4), strides=2, padding='same'),\n        LeakyReLU(alpha=0.2),\n\n        Conv2DTranspose(128, (4, 4), strides=2, padding='same'),\n        LeakyReLU(alpha=0.2),\n\n        Conv2D(CHANNELS, (4, 4), padding='same', activation='tanh')\n    ], \n    name=\"generator\")\n    model.summary()\n    model.compile(loss=\"binary_crossentropy\", optimizer=OPTIMIZER)\n\n    return model","metadata":{"id":"hqNmekiQvutW","execution":{"iopub.status.busy":"2024-11-28T21:53:07.636514Z","iopub.execute_input":"2024-11-28T21:53:07.636868Z","iopub.status.idle":"2024-11-28T21:53:07.643464Z","shell.execute_reply.started":"2024-11-28T21:53:07.636840Z","shell.execute_reply":"2024-11-28T21:53:07.642396Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_discriminator():\n    \n    \"\"\"\n        Discriminator is the model which is responsible for classifying the generated images\n        as fake or real. Our end goal is to create a Generator so powerful that the Discriminator\n        is unable to classify real and fake images\n        A simple Convolutional Neural Network with 2 Conv2D layers connected to a Dense output layer\n        Output layer activation is Sigmoid since this is a Binary Classifier\n\n        Input: Generated / Real Image\n        Output: Validity of Image (Fake or Real)\n\n    \"\"\"\n\n    model = Sequential([\n\n        Conv2D(64, (3, 3), padding='same', input_shape=(WIDTH, HEIGHT, CHANNELS)),\n        LeakyReLU(alpha=0.2),\n\n        Conv2D(128, (3, 3), strides=2, padding='same'),\n        LeakyReLU(alpha=0.2),\n\n        Conv2D(128, (3, 3), strides=2, padding='same'),\n        LeakyReLU(alpha=0.2),\n        \n        Conv2D(256, (3, 3), strides=2, padding='same'),\n        LeakyReLU(alpha=0.2),\n        \n        Flatten(),\n        Dropout(0.4),\n        Dense(1, activation=\"sigmoid\", input_shape=(WIDTH, HEIGHT, CHANNELS))\n    ], name=\"discriminator\")\n    model.summary()\n    model.compile(loss=\"binary_crossentropy\",\n                        optimizer=OPTIMIZER)\n\n    return model","metadata":{"id":"xbCbfY8EvsiT","execution":{"iopub.status.busy":"2024-11-28T21:53:07.974181Z","iopub.execute_input":"2024-11-28T21:53:07.974901Z","iopub.status.idle":"2024-11-28T21:53:07.981827Z","shell.execute_reply.started":"2024-11-28T21:53:07.974861Z","shell.execute_reply":"2024-11-28T21:53:07.980836Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('\\n')\ndiscriminator = build_discriminator()\nprint('\\n')\ngenerator = build_generator()\n\ndiscriminator.trainable = False \n\ngan_input = Input(shape=(NOISE_DIM,))\nfake_image = generator(gan_input)\n\ngan_output = discriminator(fake_image)\n\ngan = Model(gan_input, gan_output, name=\"gan_model\")\ngan.compile(loss=\"binary_crossentropy\", optimizer=OPTIMIZER)\n\nprint(\"The Combined Network:\\n\")\ngan.summary()","metadata":{"id":"6smhRLnsvuqY","execution":{"iopub.status.busy":"2024-11-28T21:53:08.822565Z","iopub.execute_input":"2024-11-28T21:53:08.823256Z","iopub.status.idle":"2024-11-28T21:53:08.981704Z","shell.execute_reply.started":"2024-11-28T21:53:08.823223Z","shell.execute_reply":"2024-11-28T21:53:08.980754Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sample_images(noise, subplots, figsize=(22, 8), save=False):\n    # Create the directory if it doesn't exist\n    output_dir = \"augmented-dataset\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    generated_images = generator.predict(noise)\n    plt.figure(figsize=figsize)\n    \n    for i, image in enumerate(generated_images):\n        if save:\n            img_name = f\"{output_dir}/gen_{i}.png\"\n            # Save individual images directly\n            if CHANNELS == 1:\n                cv2.imwrite(img_name, (image.reshape((WIDTH, HEIGHT)) * 255).astype(np.uint8))\n            else:\n                cv2.imwrite(img_name, (image.reshape((WIDTH, HEIGHT, CHANNELS)) * 255).astype(np.uint8))\n        \n        # For plotting\n        plt.subplot(subplots[0], subplots[1], i + 1)\n        if CHANNELS == 1:\n            plt.imshow(image.reshape((WIDTH, HEIGHT)), cmap='gray')    \n        else:\n            plt.imshow(image.reshape((WIDTH, HEIGHT, CHANNELS)))\n        plt.subplots_adjust(wspace=None, hspace=None)\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n","metadata":{"id":"JDm4yLl5hwFX","execution":{"iopub.status.busy":"2024-11-28T21:53:09.386983Z","iopub.execute_input":"2024-11-28T21:53:09.388039Z","iopub.status.idle":"2024-11-28T21:53:09.396537Z","shell.execute_reply.started":"2024-11-28T21:53:09.388004Z","shell.execute_reply":"2024-11-28T21:53:09.395715Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Training the DCGAN","metadata":{}},{"cell_type":"code","source":"np.random.seed(SEED)\nfor epoch in range(10):\n    for batch in tqdm(range(STEPS_PER_EPOCH)):\n\n        noise = np.random.normal(0,1, size=(BATCH_SIZE, NOISE_DIM))\n        fake_X = generator.predict(noise)\n        \n        idx = np.random.randint(0, X_train.shape[0], size=BATCH_SIZE)\n        real_X = X_train[idx]\n\n        X = np.concatenate((real_X, fake_X))\n\n        disc_y = np.zeros(2*BATCH_SIZE)\n        disc_y[:BATCH_SIZE] = 1\n\n        d_loss = discriminator.train_on_batch(X, disc_y)\n        \n        y_gen = np.ones(BATCH_SIZE)\n        g_loss = gan.train_on_batch(noise, y_gen)\n\n    print(f\"EPOCH: {epoch + 1} Generator Loss: {g_loss:.4f} Discriminator Loss: {d_loss:.4f}\")\n    noise = np.random.normal(0, 1, size=(10,NOISE_DIM))\n    sample_images(noise, (2,5))","metadata":{"id":"V6hFoFDfvunl","execution":{"iopub.status.busy":"2024-11-28T21:53:21.265336Z","iopub.execute_input":"2024-11-28T21:53:21.266437Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Generating Images","metadata":{}},{"cell_type":"code","source":"noise = np.random.normal(0, 1, size=(120, NOISE_DIM))\nsample_images(noise, (10, 12), (24, 20), save=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T11:39:36.175671Z","iopub.execute_input":"2023-03-29T11:39:36.176119Z","iopub.status.idle":"2023-03-29T11:40:45.58117Z","shell.execute_reply.started":"2023-03-29T11:39:36.176082Z","shell.execute_reply":"2023-03-29T11:40:45.576802Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: Testing the Generated sample: Plotting the Distributions¶\r\n\r\nIn this test, we compare the generated images with the real samples by plotting their distributions. If the distributions overlap, that indicates the generated samples are very close to the real ones","metadata":{}},{"cell_type":"code","source":"generated_images = generator.predict(noise)\ngenerated_images.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-29T11:41:09.67433Z","iopub.execute_input":"2023-03-29T11:41:09.674698Z","iopub.status.idle":"2023-03-29T11:41:09.818348Z","shell.execute_reply.started":"2023-03-29T11:41:09.674667Z","shell.execute_reply":"2023-03-29T11:41:09.814485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=1, nrows=1, figsize=(18,10))\n\nsns.distplot(X_train, label='Real Images', hist=True, color='#fc0328', ax=axs)\nsns.distplot(generated_images, label='Generated Images', hist=True, color='#0c06c7', ax=axs)\n\naxs.legend(loc='upper right', prop={'size': 12})\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T11:41:11.202655Z","iopub.execute_input":"2023-03-29T11:41:11.203121Z","iopub.status.idle":"2023-03-29T11:41:18.561659Z","shell.execute_reply.started":"2023-03-29T11:41:11.203082Z","shell.execute_reply":"2023-03-29T11:41:18.560668Z"},"trusted":true},"outputs":[],"execution_count":null}]}